# 758T_PredictiveModeling

# Executive Summary:
## Background and objective: 

The Johns Hopkins University School of Medicine (JHU SoM) has collected an extensive set of patient data related to treatment and discharge, and have requested analysis revolving around predicting if a discharged patient will return within 30 days. Exploration of this phenomenon could reveal a variety of actionable insights, including improved hospital scheduling and treatment plans to match forecasted patient returns. 
## Exploratory data analysis: 

We have explored the training data and addressed many areas of interest. There are both categorical and continuous numerical variables present, and the most common class for the target variable of “Return” was noted to be “No”, representing approximately 75% of the roughly 40,000 observations. There are 26 features, of varying veracity: missing categorical values that appear to not be missing at random have been replaced with a new category of “Missing”, while observations missing values in “Return” were omitted in model-building. We also noted significant correlation between pairs of related variables, and omitted the less significant of each pair to avoid using unnecessary variables. Arbitrary indexing-variables were likewise omitted before modeling.
## Modeling, in summary:

Given that baseline accuracy with the most common class (“No” for Return) is already 75.02%, we considered the importance of other metrics - specifically true positive rate (due to the relative rarity of the positive class) - in model evaluation, although accuracy was still considered as well. Models were selected for highest TPR provided they also beat baseline accuracy. Our data was engineered before model training since some categorical variables featured an extensive amount of categories, many of which were uncommon. To address this, categories were encoded into one of three bins, relative to if its associated return rate was above, at, or below the modal return rate. After modification, our data was partitioned into 20% testing, 16% validation, and 64% training sets

Principal component analysis was applied to our categorical variables and used to produce 50 principal components, which were then merged back with the continuous variables. We produced many models with a variety of methods (logistic regression, LASSO, Ridge, stepwise, k-NN, bagging, and random forest). The model with the highest TPR and above baseline accuracy was produced from 50 bagged trees, and had a 17.25% TPR and 77.5% testing accuracy. Cutoffs were then iteratively changed, and a model produced by 50 bagged logistic regressions with a cutoff of 0.38 had a TPR of 31.05%, near double that of the previous best TPR, and a testing accuracy of 76.03%. We also produced and evaluated the same variety of models on the original data (without PCA) for comparison and found a random forest of 300 trees that tried 5 variables per split had a TPR of 36.60% and testing accuracy of 75.26%. The bagged trees model performed best on the withheld data: its accuracy was 77.73%, our highest on that set.
## Conclusions: 

A tradeoff between TPR and accuracy were clearly evident in our process, as expected. In practice, the model we would use is the bagged trees model, since it had the highest testing accuracy on the withheld data. Further insights came from variable importance plots, which identified age, insurance status, discharge result, and gender to be the most important variables - all of which have intuitive rationales for affecting return rates, and perhaps could be explored from an inferential perspective in a future analysis to gain more insight into the domain knowledge of this challenge. This in turn could then be applied to feature engineering for another, and this time more-informed, foray into this predictive task.
